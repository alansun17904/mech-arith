{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "import codecs\n",
    "import re\n",
    "\n",
    "n_digits = 8\n",
    "\n",
    "# List of specific problems to skip\n",
    "problems_to_skip = [\n",
    "    \"1000 + 1000 = 2000\",\n",
    "    \"520 + 890 = 1410\",\n",
    "    \"100 + 200 = 300\",\n",
    "    \"1000 + 100 = 1100\"\n",
    "]\n",
    "\n",
    "# Load the tokenizer for the model\n",
    "model_name = \"google/gemma-2-2b-it\"  # Update with the actual model you're using\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_expected_tokens(problem):\n",
    "    \"\"\"\n",
    "    Generate the expected tokens based on the addition problem and the values of x and y.\n",
    "    Example: For x = 6, y = 1, and the problem \"532901 + 5 = 532906\", the expected tokens are:\n",
    "    ['<bos>', '5', '3', '2', '9', '0', '1', '_+', '_', '5', '_=', '_', '5', '3', '2', '9', '0', '6']\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Split the problem into the x_term and the rest, ensuring it has the format \"x_term + y_term = result\"\n",
    "        x_y_part, result_part = problem.split(\" = \")\n",
    "        x_term, y_term = x_y_part.split(\" + \")\n",
    "        \n",
    "        # Expected token pattern (every digit separated, and with the necessary symbols)\n",
    "        expected_tokens = ['<bos>'] + list(x_term) + ['▁+', '▁'] + list(y_term) + ['▁=', '▁'] + list(result_part)\n",
    "    except ValueError as e:\n",
    "        print(f\"Error parsing problem: {problem}, error: {e}\")\n",
    "        return []\n",
    "\n",
    "    return expected_tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to sanitize tokens for comparison\n",
    "def sanitize_tokens(tokens):\n",
    "    \"\"\"\n",
    "    Cleans up the token list to remove any unwanted characters or whitespace issues.\n",
    "    \"\"\"\n",
    "    return [token.strip() for token in tokens if token.strip()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def should_skip_problem(x, y, problem):\n",
    "    # Skip if the problem is in the list of specific problems to skip\n",
    "    if problem in problems_to_skip:\n",
    "        return True\n",
    "    \n",
    "    # Skip if the problem doesn't follow the format (only digits, '+', '=', and spaces)\n",
    "    # Regex: Only allow digits, +, =, and spaces\n",
    "    if not re.match(r'^\\d+ \\+ \\d+ = \\d+$', problem):\n",
    "        return True\n",
    "\n",
    "    # Ensure that the problem is an x-digit by y-digit problem\n",
    "    try:\n",
    "        x_term, y_term, result = re.split(r' \\+ | = ', problem)\n",
    "        if len(x_term) != x or len(y_term) != y:\n",
    "            return True\n",
    "    except ValueError:\n",
    "        return True\n",
    "\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to tokenize a list of problems and check if the tokenization matches the expected format\n",
    "def tokenize_problems_and_check(x, y, file_path):\n",
    "    with codecs.open(file_path, 'r', 'utf-8', 'ignore') as file:\n",
    "        problems = file.readlines()\n",
    "\n",
    "    # Create an output folder and file path\n",
    "    output_folder = f\"{max(x, y)}_problems\"\n",
    "    output_file_path = f\"{output_folder}/{x}_{y}_tokens.txt\"\n",
    "\n",
    "    passed_count = 0\n",
    "    total_count = 0\n",
    "\n",
    "    with open(output_file_path, 'w') as output_file:\n",
    "        for problem in problems:\n",
    "            problem = problem.strip()\n",
    "\n",
    "            if problem:\n",
    "                # Skip problems that do not match the format or specific problems\n",
    "                if should_skip_problem(x, y, problem):\n",
    "                    output_file.write(f\"Skipping problem: {problem}\\n\")\n",
    "                    continue\n",
    "                \n",
    "                total_count += 1\n",
    "                # Tokenize the problem\n",
    "                tokenized_output = tokenizer(problem, return_tensors=\"pt\")\n",
    "\n",
    "                # Convert the token IDs back to tokens\n",
    "                tokens = tokenizer.convert_ids_to_tokens(tokenized_output['input_ids'][0])\n",
    "\n",
    "                # Generate the expected tokens based on the values of x and y\n",
    "                expected_tokens = generate_expected_tokens(problem)\n",
    "\n",
    "                if not expected_tokens:\n",
    "                    output_file.write(f\"Problem: {problem}\\nError: Problem format is incorrect.\\n\\n\")\n",
    "                    continue\n",
    "\n",
    "                # Sanitize both actual and expected tokens for comparison\n",
    "                actual_tokens = sanitize_tokens(tokens)\n",
    "                expected_tokens = sanitize_tokens(expected_tokens)\n",
    "\n",
    "                # Compare actual tokens to expected tokens\n",
    "                if actual_tokens == expected_tokens:\n",
    "                    passed_count += 1\n",
    "\n",
    "                # Write the results to the output file\n",
    "                output_file.write(f\"accessing: {file_path}\\n\")\n",
    "                output_file.write(f\"Problem: {problem}\\n\")\n",
    "                output_file.write(f\"Tokens: {tokens}\\n\")\n",
    "                output_file.write(f\"Expected Tokens: {expected_tokens}\\n\")\n",
    "                output_file.write(f\"Tokenization match: {'PASS' if actual_tokens == expected_tokens else 'FAIL'}\\n\\n\")\n",
    "\n",
    "    # Calculate the percentage of problems that passed the tokenization check\n",
    "    percentage_passed = (passed_count / total_count) * 100 if total_count > 0 else 0\n",
    "    print(f\"Percentage of correctly tokenized problems for x = {x} and y = {y}: {percentage_passed:.2f}%\")\n",
    "\n",
    "    return percentage_passed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of correctly tokenized problems for x = 1 and y = 1: 100.00%\n",
      "Percentage of correctly tokenized problems for x = 1 and y = 1: 100.00%\n",
      "Percentage of correctly tokenized problems for x = 2 and y = 1: 100.00%\n",
      "Percentage of correctly tokenized problems for x = 1 and y = 2: 100.00%\n",
      "Percentage of correctly tokenized problems for x = 2 and y = 2: 100.00%\n",
      "Percentage of correctly tokenized problems for x = 2 and y = 2: 100.00%\n",
      "Percentage of correctly tokenized problems for x = 3 and y = 1: 100.00%\n",
      "Percentage of correctly tokenized problems for x = 1 and y = 3: 100.00%\n",
      "Percentage of correctly tokenized problems for x = 3 and y = 2: 100.00%\n",
      "Percentage of correctly tokenized problems for x = 2 and y = 3: 100.00%\n",
      "Percentage of correctly tokenized problems for x = 3 and y = 3: 100.00%\n",
      "Percentage of correctly tokenized problems for x = 3 and y = 3: 100.00%\n",
      "Percentage of correctly tokenized problems for x = 4 and y = 1: 100.00%\n",
      "Percentage of correctly tokenized problems for x = 1 and y = 4: 100.00%\n",
      "Percentage of correctly tokenized problems for x = 4 and y = 2: 100.00%\n",
      "Percentage of correctly tokenized problems for x = 2 and y = 4: 100.00%\n",
      "Percentage of correctly tokenized problems for x = 4 and y = 3: 100.00%\n",
      "Percentage of correctly tokenized problems for x = 3 and y = 4: 100.00%\n",
      "Percentage of correctly tokenized problems for x = 4 and y = 4: 100.00%\n",
      "Percentage of correctly tokenized problems for x = 4 and y = 4: 100.00%\n",
      "Percentage of correctly tokenized problems for x = 5 and y = 1: 100.00%\n",
      "Percentage of correctly tokenized problems for x = 1 and y = 5: 100.00%\n",
      "Percentage of correctly tokenized problems for x = 5 and y = 2: 100.00%\n",
      "Percentage of correctly tokenized problems for x = 2 and y = 5: 100.00%\n",
      "Percentage of correctly tokenized problems for x = 5 and y = 3: 100.00%\n",
      "Percentage of correctly tokenized problems for x = 3 and y = 5: 100.00%\n",
      "Percentage of correctly tokenized problems for x = 5 and y = 4: 100.00%\n",
      "Percentage of correctly tokenized problems for x = 4 and y = 5: 100.00%\n",
      "Percentage of correctly tokenized problems for x = 5 and y = 5: 100.00%\n",
      "Percentage of correctly tokenized problems for x = 5 and y = 5: 100.00%\n",
      "Percentage of correctly tokenized problems for x = 6 and y = 1: 100.00%\n",
      "Percentage of correctly tokenized problems for x = 1 and y = 6: 100.00%\n",
      "Percentage of correctly tokenized problems for x = 6 and y = 2: 100.00%\n",
      "Percentage of correctly tokenized problems for x = 2 and y = 6: 100.00%\n",
      "Percentage of correctly tokenized problems for x = 6 and y = 3: 100.00%\n",
      "Percentage of correctly tokenized problems for x = 3 and y = 6: 100.00%\n",
      "Percentage of correctly tokenized problems for x = 6 and y = 4: 100.00%\n",
      "Percentage of correctly tokenized problems for x = 4 and y = 6: 100.00%\n",
      "Percentage of correctly tokenized problems for x = 6 and y = 5: 100.00%\n",
      "Percentage of correctly tokenized problems for x = 5 and y = 6: 100.00%\n",
      "Percentage of correctly tokenized problems for x = 6 and y = 6: 100.00%\n",
      "Percentage of correctly tokenized problems for x = 6 and y = 6: 100.00%\n",
      "Percentage of correctly tokenized problems for x = 7 and y = 1: 100.00%\n",
      "Percentage of correctly tokenized problems for x = 1 and y = 7: 100.00%\n",
      "Percentage of correctly tokenized problems for x = 7 and y = 2: 100.00%\n",
      "Percentage of correctly tokenized problems for x = 2 and y = 7: 100.00%\n",
      "Percentage of correctly tokenized problems for x = 7 and y = 3: 100.00%\n",
      "Percentage of correctly tokenized problems for x = 3 and y = 7: 100.00%\n",
      "Percentage of correctly tokenized problems for x = 7 and y = 4: 100.00%\n",
      "Percentage of correctly tokenized problems for x = 4 and y = 7: 100.00%\n",
      "Percentage of correctly tokenized problems for x = 7 and y = 5: 100.00%\n",
      "Percentage of correctly tokenized problems for x = 5 and y = 7: 100.00%\n",
      "Percentage of correctly tokenized problems for x = 7 and y = 6: 100.00%\n",
      "Percentage of correctly tokenized problems for x = 6 and y = 7: 100.00%\n",
      "Percentage of correctly tokenized problems for x = 7 and y = 7: 100.00%\n",
      "Percentage of correctly tokenized problems for x = 7 and y = 7: 100.00%\n",
      "Percentage of correctly tokenized problems for x = 8 and y = 1: 100.00%\n",
      "Percentage of correctly tokenized problems for x = 1 and y = 8: 100.00%\n",
      "Percentage of correctly tokenized problems for x = 8 and y = 2: 100.00%\n",
      "Percentage of correctly tokenized problems for x = 2 and y = 8: 100.00%\n",
      "Percentage of correctly tokenized problems for x = 8 and y = 3: 100.00%\n",
      "Percentage of correctly tokenized problems for x = 3 and y = 8: 100.00%\n",
      "Percentage of correctly tokenized problems for x = 8 and y = 4: 100.00%\n",
      "Percentage of correctly tokenized problems for x = 4 and y = 8: 100.00%\n",
      "Percentage of correctly tokenized problems for x = 8 and y = 5: 100.00%\n",
      "Percentage of correctly tokenized problems for x = 5 and y = 8: 100.00%\n",
      "Percentage of correctly tokenized problems for x = 8 and y = 6: 100.00%\n",
      "Percentage of correctly tokenized problems for x = 6 and y = 8: 100.00%\n",
      "Percentage of correctly tokenized problems for x = 8 and y = 7: 100.00%\n",
      "Percentage of correctly tokenized problems for x = 7 and y = 8: 100.00%\n",
      "Percentage of correctly tokenized problems for x = 8 and y = 8: 100.00%\n",
      "Percentage of correctly tokenized problems for x = 8 and y = 8: 100.00%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for i in range(1, n_digits+1):\n",
    "    for j in range(0, i+1):\n",
    "        if j == 1:\n",
    "            continue\n",
    "        if j == 0:\n",
    "            j += 1\n",
    "        \n",
    "        folder = f\"{max(i, j)}_results\"\n",
    "        x_file_path = f\"{folder}/{i}_by_{j}_results/{i}_by_{j}_at_1.0_results.pkl\"\n",
    "        y_file_path = f\"{folder}/{j}_by_{i}_results/{j}_by_{i}_at_1.0_results.pkl\"\n",
    "\n",
    "        # Call the function to tokenize and retrieve the tokens from the file\n",
    "        tokenize_problems_and_check(i, j, x_file_path)\n",
    "        tokenize_problems_and_check(j, i, y_file_path)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
