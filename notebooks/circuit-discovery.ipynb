{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "536bf6db-1449-4854-b567-f9070ecfd598",
   "metadata": {},
   "source": [
    "# Single Digit Add Memorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c6072c3a-fe13-4cb0-b040-61e35c7cfcc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8c80273c-699a-444d-a73a-4cca56661a68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformer_lens.patching as patching\n",
    "from transformer_lens import HookedTransformer, ActivationCache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ca229a81-be9e-4301-b928-e139c4a9edd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformer_lens.utils as utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "ab9ff851-c0b2-417f-a5ee-9abf96147a46",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ae325d66-aae9-4861-a385-66daf6bcbb8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed: int):    \n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e2da8d95-9083-444b-9f9b-8c46ab394a97",
   "metadata": {},
   "outputs": [],
   "source": [
    "def arith_probs(dig1: int, dig2: int, n=1000, sub=False):\n",
    "    \"\"\"Generate binary arithmetic problems (both addition and subtraction).\n",
    "\n",
    "    Args:\n",
    "        dig1 (int): the number of digits in the first operand.\n",
    "        dig2 (int): the number of digits in the second operand.\n",
    "        n (int): the total number of problems.\n",
    "        sub (bool): if true, the operation of the problem becomes subtraction;\n",
    "            otherwise, generates only addition problems.\n",
    "    Returns:\n",
    "        a list of tuple of three integers (op1, op2, ans) where op1, op2 \n",
    "        have dig1, dig2 number of digits, respectively; \n",
    "        and op1 +/- op2 = ans. Note that op1 >= 0 and op2 >= 0.\n",
    "    \"\"\"\n",
    "    probs = []\n",
    "    for _ in range(n):\n",
    "        a = random.randint(10 ** (dig1 - 1), 10 ** dig1 - 1)\n",
    "        b = random.randint(10 ** (dig2 - 1), 10 ** dig2 - 1)\n",
    "        ans = a + b if not sub else a - b\n",
    "        probs.append((a,b,ans))\n",
    "    return probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4c403b1e-8fbc-4183-bfff-abf287e58260",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fewshot_probs(probs, sub=False, k=2):\n",
    "    \"\"\"Tokenize a list of problems in the form of (a, b, ans).\n",
    "    \n",
    "    Args:\n",
    "        tokenizer: model tokenizer\n",
    "        probs (List[Tuple[int, int, int]]): a list of tuples of the form\n",
    "            (a, b, ans). \n",
    "        sub (bool): whether the problem is subtract\n",
    "        k (int): number of examples given to the model for in-context learning\n",
    "    Returns:\n",
    "        list of problems have been tokenized. \n",
    "    \"\"\"\n",
    "    convert_few = lambda x: f\"{x[0]} {'-' if sub else '+'} {x[1]} = {x[2]}\"\n",
    "    str_probs = []\n",
    "    for i, p in enumerate(probs):\n",
    "        # sample the few shot problems\n",
    "        few_shot_examples = [convert_few(v) for v in random.sample(probs, k=k)]\n",
    "        # str_probs.append(\"100 + 200 = 300\\n520 + 890 = 1410\" + \"\\n\" + convert_prob(p))\n",
    "        str_probs.append(\"\\n\".join(few_shot_examples) + \"\\n\" + convert_few(p))\n",
    "    # return str_probs\n",
    "    return str_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "03f841f9-9151-4a71-a4a4-c729c7ac9dac",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_everything(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "02460c78-1b27-4761-a807-24cd170c6b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "toks = AutoTokenizer.from_pretrained(\"google/gemma-2-2b\", padding_side=\"left\")\n",
    "toks.pad_token = toks.bos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d016575f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`config.hidden_act` is ignored, you should use `config.hidden_activation` instead.\n",
      "Gemma's activation function will be set to `gelu_pytorch_tanh`. Please, use\n",
      "`config.hidden_activation` if you want to override this behaviour.\n",
      "See https://github.com/huggingface/transformers/pull/29402 for more details.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1ccdc90a7f0453dba6efc679e4222bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:You are not using LayerNorm, so the writing weights can't be centered! Skipping\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gemma-2b into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "m = HookedTransformer.from_pretrained(\"gemma-2b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3c8a5620-a135-416d-940b-2a299a6f2fff",
   "metadata": {},
   "outputs": [],
   "source": [
    "probs = arith_probs(1,1,n=100)\n",
    "fewshot_probs = fewshot_probs(probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "72efa58f-0ca4-48a6-b45d-ff68fde976d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_logit_indices(tokenized_input, problems):\n",
    "    # get the first index of the answer in each problem\n",
    "    cindices = list(map(lambda x: x.rfind(\" \") + 1, problems))\n",
    "    indices = []\n",
    "    answer_indices = []\n",
    "    for i in range(len(problems)):\n",
    "        om = tokenized_input[\"offset_mapping\"][i]\n",
    "        counter = len(om)-1\n",
    "        idxs = []\n",
    "        ans_idxs = []\n",
    "        for j in range(len(om)-1,-1,-1):\n",
    "            if om[j][0] >= cindices[i]:\n",
    "                idxs.append(counter)\n",
    "                ans_idxs.append(tokenized_input[\"input_ids\"][i][counter].item())\n",
    "                counter -= 1\n",
    "            else:\n",
    "                break\n",
    "        indices.append(idxs)\n",
    "        answer_indices.append(ans_idxs)\n",
    "    return indices, answer_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "0e7d9743-00d9-445b-9950-bb4e4cd09ea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_input = toks(fewshot_probs, return_offsets_mapping=True, padding=True, return_tensors=\"pt\")\n",
    "idxs, ans_idx = answer_logit_indices(tokenized_input, fewshot_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "49622b9a-fbdb-43e8-bc46-864898b67461",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pad both idx and ans_idxs with zeros\n",
    "maxlen = lambda x: max(x, key=lambda y: len(y))\n",
    "max_idx_len, ans_idx_len = maxlen(idxs), maxlen(ans_idxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "cb65bdcd-7002-4adb-b76e-097d46176ae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "mil = len(max_idx_len)\n",
    "ail = len(ans_idxs_len)\n",
    "for i in range(len(idxs)):\n",
    "    if len(idxs[i]) < mil:\n",
    "        idxs[i] = [0] * (mil - len(idxs[i])) + idxs[i]\n",
    "    if len(ans_idx[i]) < ail:\n",
    "        ans_idx[i] = [0] * (ail - len(ans_idx[i])) + ans_idx[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "62dbd7ca-189d-4bdd-aa02-b9043192cccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "idxs, ans_idx = torch.LongTensor(idxs), torch.LongTensor(ans_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "de5d5abb-892d-440b-9533-85e7f28eaeda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100, 100, 2, 256000])"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clogits_noise[:,idxs,:].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "0538d14a-36e6-4921-a93c-5b86f469ca5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def corr_hook_noise(clean, hook):\n",
    "    che = torch.std(clean).to(\"cpu\") * 3\n",
    "    clean = clean + torch.normal(torch.zeros(clean.shape), che * torch.ones(clean.shape)).to(clean.device)\n",
    "    return clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "c424df33-4553-4af9-8294-e603ba4db9f7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 170.00 MiB. GPU 0 has a total capacity of 44.34 GiB of which 138.81 MiB is free. Including non-PyTorch memory, this process has 44.20 GiB memory in use. Of the allocated memory 43.25 GiB is allocated by PyTorch, and 653.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[52], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m clean_logits \u001b[38;5;241m=\u001b[39m m(tokenized_input[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "File \u001b[0;32m/BRAIN/circuit-alignment/work/miniconda3/envs/ca-dgx1/lib/python3.12/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/BRAIN/circuit-alignment/work/miniconda3/envs/ca-dgx1/lib/python3.12/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/BRAIN/circuit-alignment/work/miniconda3/envs/ca-dgx1/lib/python3.12/site-packages/transformer_lens/HookedTransformer.py:561\u001b[0m, in \u001b[0;36mHookedTransformer.forward\u001b[0;34m(self, input, return_type, loss_per_token, prepend_bos, padding_side, start_at_layer, tokens, shortformer_pos_embed, attention_mask, stop_at_layer, past_kv_cache)\u001b[0m\n\u001b[1;32m    556\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m shortformer_pos_embed \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    557\u001b[0m         shortformer_pos_embed \u001b[38;5;241m=\u001b[39m shortformer_pos_embed\u001b[38;5;241m.\u001b[39mto(\n\u001b[1;32m    558\u001b[0m             devices\u001b[38;5;241m.\u001b[39mget_device_for_block_index(i, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcfg)\n\u001b[1;32m    559\u001b[0m         )\n\u001b[0;32m--> 561\u001b[0m     residual \u001b[38;5;241m=\u001b[39m block(\n\u001b[1;32m    562\u001b[0m         residual,\n\u001b[1;32m    563\u001b[0m         \u001b[38;5;66;03m# Cache contains a list of HookedTransformerKeyValueCache objects, one for each\u001b[39;00m\n\u001b[1;32m    564\u001b[0m         \u001b[38;5;66;03m# block\u001b[39;00m\n\u001b[1;32m    565\u001b[0m         past_kv_cache_entry\u001b[38;5;241m=\u001b[39mpast_kv_cache[i] \u001b[38;5;28;01mif\u001b[39;00m past_kv_cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    566\u001b[0m         shortformer_pos_embed\u001b[38;5;241m=\u001b[39mshortformer_pos_embed,\n\u001b[1;32m    567\u001b[0m         attention_mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[1;32m    568\u001b[0m     )  \u001b[38;5;66;03m# [batch, pos, d_model]\u001b[39;00m\n\u001b[1;32m    570\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m stop_at_layer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    571\u001b[0m     \u001b[38;5;66;03m# When we stop at an early layer, we end here rather than doing further computation\u001b[39;00m\n\u001b[1;32m    572\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m residual\n",
      "File \u001b[0;32m/BRAIN/circuit-alignment/work/miniconda3/envs/ca-dgx1/lib/python3.12/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/BRAIN/circuit-alignment/work/miniconda3/envs/ca-dgx1/lib/python3.12/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/BRAIN/circuit-alignment/work/miniconda3/envs/ca-dgx1/lib/python3.12/site-packages/transformer_lens/components/transformer_block.py:181\u001b[0m, in \u001b[0;36mTransformerBlock.forward\u001b[0;34m(self, resid_pre, shortformer_pos_embed, past_kv_cache_entry, attention_mask)\u001b[0m\n\u001b[1;32m    177\u001b[0m     mlp_in \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    178\u001b[0m         resid_mid \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcfg\u001b[38;5;241m.\u001b[39muse_hook_mlp_in \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhook_mlp_in(resid_mid\u001b[38;5;241m.\u001b[39mclone())\n\u001b[1;32m    179\u001b[0m     )\n\u001b[1;32m    180\u001b[0m     normalized_resid_mid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mln2(mlp_in)\n\u001b[0;32m--> 181\u001b[0m     mlp_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_mlp(normalized_resid_mid)\n\u001b[1;32m    182\u001b[0m     resid_post \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhook_resid_post(resid_mid \u001b[38;5;241m+\u001b[39m mlp_out)  \u001b[38;5;66;03m# [batch, pos, d_model]\u001b[39;00m\n\u001b[1;32m    183\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcfg\u001b[38;5;241m.\u001b[39mparallel_attn_mlp:\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;66;03m# Dumb thing done by GPT-J, both MLP and Attn read from resid_pre and write to resid_post, no resid_mid used.\u001b[39;00m\n\u001b[1;32m    185\u001b[0m     \u001b[38;5;66;03m# In GPT-J, LN1 and LN2 are tied, in GPT-NeoX they aren't.\u001b[39;00m\n",
      "File \u001b[0;32m/BRAIN/circuit-alignment/work/miniconda3/envs/ca-dgx1/lib/python3.12/site-packages/transformer_lens/components/transformer_block.py:205\u001b[0m, in \u001b[0;36mTransformerBlock.apply_mlp\u001b[0;34m(self, normalized_resid)\u001b[0m\n\u001b[1;32m    197\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply_mlp\u001b[39m(\n\u001b[1;32m    198\u001b[0m     \u001b[38;5;28mself\u001b[39m, normalized_resid: Float[torch\u001b[38;5;241m.\u001b[39mTensor, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch pos d_model\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    199\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Float[torch\u001b[38;5;241m.\u001b[39mTensor, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch pos d_model\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[1;32m    200\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Centralized point where the MLP is applied to the forward pass\u001b[39;00m\n\u001b[1;32m    201\u001b[0m \n\u001b[1;32m    202\u001b[0m \u001b[38;5;124;03m    Returns:\u001b[39;00m\n\u001b[1;32m    203\u001b[0m \u001b[38;5;124;03m        Float[torch.Tensor, \"batch pos d_model\"]: Our resulting tensor\u001b[39;00m\n\u001b[1;32m    204\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 205\u001b[0m     mlp_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmlp(normalized_resid)  \u001b[38;5;66;03m# [batch, pos, d_model]\u001b[39;00m\n\u001b[1;32m    206\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcfg\u001b[38;5;241m.\u001b[39muse_normalization_before_and_after:\n\u001b[1;32m    207\u001b[0m         mlp_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mln2_post(mlp_out)\n",
      "File \u001b[0;32m/BRAIN/circuit-alignment/work/miniconda3/envs/ca-dgx1/lib/python3.12/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/BRAIN/circuit-alignment/work/miniconda3/envs/ca-dgx1/lib/python3.12/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/BRAIN/circuit-alignment/work/miniconda3/envs/ca-dgx1/lib/python3.12/site-packages/transformer_lens/components/mlps/gated_mlp.py:70\u001b[0m, in \u001b[0;36mGatedMLP.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     65\u001b[0m     pre_linear \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhook_pre_linear(\n\u001b[1;32m     66\u001b[0m         torch\u001b[38;5;241m.\u001b[39mmatmul(x, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mW_in)  \u001b[38;5;66;03m# batch pos d_model, d_model d_mlp -> batch pos d_mlp\u001b[39;00m\n\u001b[1;32m     67\u001b[0m     )\n\u001b[1;32m     69\u001b[0m     post_act \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhook_post(\n\u001b[0;32m---> 70\u001b[0m         (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mact_fn(pre_act) \u001b[38;5;241m*\u001b[39m pre_linear) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mb_in\n\u001b[1;32m     71\u001b[0m     )  \u001b[38;5;66;03m# [batch, pos, d_mlp]\u001b[39;00m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m batch_addmm(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mb_out, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mW_out, post_act)\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 170.00 MiB. GPU 0 has a total capacity of 44.34 GiB of which 138.81 MiB is free. Including non-PyTorch memory, this process has 44.20 GiB memory in use. Of the allocated memory 43.25 GiB is allocated by PyTorch, and 653.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "clean_logits, clean_cache = m.run_with_cache(tokenized_input[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "eafa2cdc-2cf7-4805-b83e-4280a78dd148",
   "metadata": {},
   "outputs": [],
   "source": [
    "with m.hooks(fwd_hooks=[(\"hook_embed\", corr_hook_noise)]):\n",
    "    noise_logits, noise_cache = m.run_with_cache(tokenized_input[\"input_ids\"])\n",
    "    noise_logits = clogits_noise.to(\"cpu\")\n",
    "    noise_cache = ccache_noise.to(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "0343374b-f44e-4e1f-9436-07e077856d10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def logit_diff(patched_logits, clean_logits, corrupted_logits, answer_token_idxs=idxs, answer_token_ids=ans_idx):\n",
    "    mask = (answer_token_idxs != 0)\n",
    "    n = torch.arange(len(answer_token_idxs)).unsqueeze(1).expand(len(answer_token_idxs), answer_token_idxs.shape[1])\n",
    "    clean_ans_logits = clean_logits[n,answer_token_idxs,answer_token_ids]\n",
    "    corr_ans_logits = corrupted_logits[n,answer_token_idxs,answer_token_ids]\n",
    "    pat_ans_logits = patched_logits[n,answer_token_idxs,answer_token_ids]\n",
    "    pert_change = (pat_ans_logits - corr_ans_logits) / (clean_ans_logits - corr_ans_logits)\n",
    "    return torch.sum(mask * pert_change) / torch.sum(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e25faed-c384-4315-9fe9-2ef15aba9543",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = partial(logit_diff("
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d834e1ab-7e6e-44e3-a64b-d656012556e7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
